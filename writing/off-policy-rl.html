<!DOCTYPE html>
<html lang="en" id="html">

<head>
    <meta charset="utf-8">
    <title>Suhas Kotha</title>
    <!-- <script src="jquery-3.6.3.min.js"></script> -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true
          },
          TeX: {
            extensions: ["AMSmath.js", "AMSsymbols.js"],
            equationNumbers: { autoNumber: "AMS" }
          },
        });
    </script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML"></script>
    </script>
    <meta name="description" content="Introduction to Off-Policy RL">
    <meta name="author" content="Suhas Kotha">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Quicksand' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="../css/style.css?v=<?php echo rand(); ?>">
    <link rel="icon" type="image/png" href="../images/dragon_logo.png">
    <style id="autogen"></style>
</head>

<body style="max-width: 100%; margin: 0 auto;">
    <p><a href=../index.html>Home</a></p>
    <div class="blog-post">
        <br>

        <h1>Off-Policy RL</h1>
        <h1>!work in progress!</h1>

        \(
        \def\R{\mathbb{R}}
        \def\wavg#1{\overline{f(x_{#1:T})}}
        \newcommand{\clip}[3]{\left[#1\right]^{#3}_{#2}}
        \)

        <!-- \newcommand{\clip}[3]{\texttt{clip}(#1, #2, #3)} -->

        <br>
        <p style="text-align: center;">6/11/2025</p>
        <br>

        <!-- Outline
         
        -->

        <!-- <figure>
            <img src="../images/permpig/linreg_fixed_perms.png" style="max-width: 95%; height: auto;">
            <figcaption>Figure 1: Training loss for different methods. $k$ is the number of data points synthesized per real data point. $k=1$ is standard training, $k>1$ is standard augmentation, and $k=\infty$ is the true expected gradient. </figcaption>
        </figure> -->

        <p>Massive credit to <a href="https://www.konwoo.kim/">Konwoo</a> for teaching me basically everything shared here.</p>

        <h3>Policy Gradient</h3>

        <p>Our goal is to maximize the expected reward of a policy $\pi_\theta$, which is given by</p>
        <p class="math">
            \begin{aligned}
            J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right]
            \end{aligned}
        </p>

        <p>for rollouts $\tau$ scored by a reward function $R$. Though there are traditionally inputs $x$ associated with the rollouts, we will suppress them for simplicity. Akin to supervised machine learning, we would like to take the derivative of $J(\theta)$ with respect to $\theta$ and update $\theta$ in the direction of the gradient. However, this is not a differentiable reward function and the naive gradient would be zero. REINFORCE constructs the policy gradient by utilizing the reparameterization trick. This utilizes the log-gradient trick to compute the gradient as shown below.</p>
        <p class="math">
            \begin{aligned}
            \nabla_\theta J(\theta) &= \nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right] \\
            &= \nabla_\theta \int_{\tau} \pi_\theta(\tau) R(\tau)  d\tau && \text{[definition of expectation]}\\
            &= \int_{\tau} \nabla_\theta \pi_\theta(\tau) R(\tau)  d\tau && \text{[swap integral and gradient]}\\
            &= \int_{\tau} \pi_\theta(\tau) \nabla_\theta \log \pi_\theta(\tau) R(\tau) d\tau && \text{[log-gradient trick, $\nabla f(x) = f(x) \nabla \log f(x)$]}\\
            &= \boxed{\mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(\tau) R(\tau) \right]} && \text{[definition of expectation]}
            \end{aligned}
        </p>
        <p>The boxed expression is the policy gradient which we want to use to update $\theta$.</p>

        <h4>Computing via autograd</h4>
            
        <p>To actually compute the policy gradient, it is standard to construct a surrogate reward function such thats PyTorch's autograd can differentiate through it. We note there are two ways to do this. The first method applies the log-gradient trick in reverse and leverages $\frac{\nabla_\theta \pi_{\theta}(\tau)}{\pi_{\theta}(\tau)}$ with the objective</p>
        <p class="math">
            \begin{equation}\label{eq:on-policy-surrogate-1}
            J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \frac{\pi_{\theta}(\tau)}{\texttt{detach}(\pi_{\theta}(\tau))} R(\tau) \right]
            \end{equation}
        </p>
        <p>where the $\texttt{detach}$ operator signifies that we are not differentiating through the denominator. This is equal in value to the expected reward but yields a different gradient. The second method directly exploits the fact that the policy gradient uses $\nabla_\theta \log \pi_\theta(\tau)$ by</p>
        <p class="math">
            \begin{equation}\label{eq:on-policy-surrogate-2}
            J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \log \pi_\theta(\tau) R(\tau) \right]
            \end{equation}
        </p> 
        <p>Note that the even though the first and second methods are not equivalent in value, their gradients are the same (both correspond to the policy gradient). Therefore, in our on-policy setting, both methods are exactly the same.</p>

        <h4>Using a baseline</h4>

        <p>Though this gradient is unbiased, we might want to reduce its variance. One way to do this is to subtract a baseline $b$ from the reward. This doesn't change the expected policy gradient, as shown below.</p>

        <p class="math">
            \begin{aligned}
            \nabla_\theta J_{b}(\theta) & = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(\tau) (R(\tau) - b) \right] \\
            & = \nabla_\theta J(\theta) - b \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(\tau) \right] && \text{[$b$ is independent of $\tau$]} \\
            & = \nabla_\theta J(\theta) - b \mathbb{E}_{\tau \sim \pi_\theta} \left[ \frac{\nabla_\theta \pi_\theta(\tau)}{\pi_\theta(\tau)} \right] && \text{[log-gradient trick]} \\
            & = \nabla_\theta J(\theta) - b \nabla_\theta\mathbb{E}_{\tau \sim \pi_\theta} \left[ \frac{\pi_\theta(\tau)}{\pi_\theta(\tau)} \right] && \text{[exchange order]} \\
            & = \nabla_\theta J(\theta) && \text{[derivative of constant]} \\
            \end{aligned}
        </p>

        <p>A common target for the baseline is the value function, or the mean reward of trajectories sampled from the policy. This can be estimated by learning a value function on the fly or by taking the mean reward of trajectories sampled from the policy in a batch. We will ignore baselines until we analyze the clipped policy gradient.</p>

        <h3>Going off-policy (via importance sampling and clipping)</h3>

        <p>From a systems perspective, the policy generating the data is different from the policy we are updating (<cite><a href="https://arxiv.org/abs/1602.01783">Mnih et al, 2016</a></cite>, <cite><a href="https://arxiv.org/abs/1802.01561">Espeholt et al, 2018</a></cite>). For example, language model inference is expensive and utilizes a different infrastructure than training (vLLM vs HuggingFace). Therefore, the sampling distribution can be different from the training distribution because it is a few steps behind, it uses different code, or is quantized for efficiency. Unfortunately, the policy gradient assumes that the data is sampled from the policy we are updating.</p>

        <br>

        <p>Off-policy RL methods are designed to handle this. The core solution is to use importance sampling to reweight the likelihood of each trajectory. Therefore, even though the trajectories are sampled from a different policy, we get an unbiased estimate of the policy gradient if we had been sampling on-policy. The importance sampled policy gradient is given by</p>
        <p class="math">
            \begin{aligned}
            \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_{\text{gen}}} \left[ \frac{\pi_\theta(\tau)}{\pi_{\text{gen}}(\tau)} \nabla_\theta \log \pi_\theta(\tau) R(\tau) \right]
            \end{aligned}
        </p>
        <p>where $\pi_{\text{gen}}$ is the policy that generated the data. In expectation, this is the same as the on-policy policy gradient. However, this estimator has incredibly high variance because the importance sampling weights can very large; this is worsened the further off-policy we are. How can we address this?</p>

        <h4>Clipping in PPO/TRPO</h4>

        <p>The solution employed by the most popular methods of Proximal Policy Optimization (PPO) <cite><a href="https://arxiv.org/abs/1707.06347">(Schulman et al. 2017)</a></cite> and Trust Region Policy Optimization (TRPO) <cite><a href="https://arxiv.org/abs/1502.05477">(Schulman et al. 2015)</a></cite> is to clip the importance sampling weights to be between 1-$\epsilon$ and 1+$\epsilon$, denoted $\clip{\frac{\pi_\theta(\tau)}{\pi_{\text{gen}}(\tau)}}{1-\epsilon}{1+\epsilon}$. On top of this, PPO uses a pessimistic bound on the reward, which means it takes the minimum reward between clipping and no clipping. This is given by the following expression adapting Equation \eqref{eq:on-policy-surrogate-1} with importance sampling, clipping, and pessimistic updates:</p>

        <p class="math">
            \begin{aligned}
            J_{\text{PPO}}(\theta) = \mathbb{E}_{\tau \sim \pi_{\text{gen}}} \left[ \min \left(\frac{\pi_\theta(\tau)}{\pi_{\text{gen}}(\tau)} R(\tau), \clip{\frac{\pi_\theta(\tau)}{\pi_{\text{gen}}(\tau)}}{1-\epsilon}{1+\epsilon} R(\tau) \right) \right]
            \end{aligned}
        </p>

        <p>Note that similar to Equation \eqref{eq:on-policy-surrogate-1}, this update leverages the fact that the numerator of the importance sampling weight $\pi_\theta(\tau)$ is differentiable whereas the denominator $\pi_{\text{gen}}(\tau)$ is detached.</p>

        

        <h4>Correct clipping for less conservative updates</h4>

        <p>The conservativeness of the PPO update is motivated by classical intuition from "trust regions" where one should not stray too far from the reference policy. In older RL settings, the generator policy is frequently reset to the current policy, promoting the use of updates that don't stray too far from the initial policy.</p>

        <br>
        
        <p>However, this can prevent going further off-policy. One striking failure mode of PPO is that for positive updates that trigger clipping, the policy will not move <b>at all</b>. This is because when clipping triggers, the importance sampling weight becomes a constant, the reward is also a constant, and the gradient becomes zero. Though this is desired under the trust region intuition, this prevents more aggressive off-policy RL. Though we still need clipping for stability, we can apply it directly to the policy gradient via the following expression:</p>
        <p class="math">
            \begin{aligned}
            \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_{\text{gen}}} \left[ \clip{\frac{\pi_\theta(\tau)}{\pi_{\text{gen}}(\tau)}}{1-\epsilon}{1+\epsilon} \nabla_\theta \log \pi_\theta(\tau) R(\tau) \right]
            \end{aligned}
        </p>
        <p>If we would like this policy gradient instead of the zero'd out gradient, we can follow Equation \eqref{eq:on-policy-surrogate-2} instead of Equation \eqref{eq:on-policy-surrogate-1} and get the following expression:</p>
        <p class="math">
            \begin{aligned}
            J_{\text{CISPO}}(\theta) = \mathbb{E}_{\tau \sim \pi_{\text{gen}}} \left[ \clip{\frac{\pi_\theta(\tau)}{\pi_{\text{gen}}(\tau)}}{1-\epsilon}{1+\epsilon} \log \pi_\theta(\tau) R(\tau) \right]
            \end{aligned}
        </p>
        <p>This is the approach followed by Minimax in their CISPO objective <cite>(<a href="https://arxiv.org/abs/2506.13585">Minimax, 2025</a>)</cite> and by Meta in their asynchronous RL implementation <cite>(<a href="https://arxiv.org/abs/2505.24034">Wu et al, 2025</a>)</cite>. In both implementations, they only clip the upper bound. This less conservative approach works much better in practice and enables going further off-policy. From here on out, we will always take clip the importance sampling ratio in the gradients, not rewards.</p>

        <!-- <h4>Toy example comparing both approaches</h4>

        <p>Let's turn off baselines and do a simple two arm bandit. Action 1 gives reward 0 and action 2 gives reward 1. We will sample trajectories from the policy $\pi_{\text{gen}}$ which places weight 0.9 on action 1 and 0.1 on action 2. We will start with learner policy $\pi_\theta$ which places weight 0.1 on action 1 and 0.9 on action 2. When the generator policy samples action 1, the importance sampling weight is $\frac{1}{9}$ and the reward is $0$. When the generator policy samples action 2, the importance sampling weight is $9$ and the reward is $1$. What will both methods give for the update?</p>

        <ol>
            <li>PPO: Since both importance sampling weights are outside typical clipping range, the update will be 0 for all trajectories. </li>
            <li>Surrogate: Both importance sampling weights will get clipped to the boundary of the clipping range $1-\epsilon^{(2)}, 1+\epsilon^{(2)}$. Therefore, both updates will happen, just more cautiously.</li>
        </ol> -->

        <h3>Clipping is biased</h3>

        <p>Though clipping induces stability by reducing variance, it makes our gradient estimator biased and inconsistent. This means that even with infinite samples per batch, we will get an incorrect estimate of the gradient and the policy will not converge to the optimal policy. Though we are generally happy to trade off some bias for variance, it is unclear that clipping is the best way to do this. </p>

        <br>

        <p>One related property is that unlike on-policy RL, shifting the reward by a constant will change the expected policy gradient. For example, suppose we introduced a constant baselines $b$. As established earlier, when clipping doesn't trigger, the policy gradient doesn't change. However, when upper-bound clipping triggers, the policy gradient gets shifted by </p>

        <p class="math">
            \begin{aligned}
            -(1+\epsilon) \nabla_\theta \log \pi_\theta(\tau) b
            \end{aligned}
        </p>

        <p>This term is not zero expectation and corresponds to a vanilla behavior cloning term. Specifically, for any clipped trajectory, maximizing this term leads to <i>imitating</i> the generator policy on the samples that triggered clipping. If $b$ is negative, this leads to <i>running away</i> from the generator policy. Therefore, calibrating the rewards is really important for clipped off-policy RL to prevent cloning, not just for stability. </p>

        <br>

        <p>TODO: empirical results of calibrating the reward shift</p>

        <h3>Potential idea: Removing clipping with smarter importance sampling</h3>

        <p>Seems like clipping is introducing a lot of issues by introducing bias and breaking shift-invariance. Is there a better way to do off-policy RL? It is well known that importance sampling isn't necessarily the lowest variance estimator. Here, we consider using self-normalized importance sampling for estimating the policy gradient. This corresponds to</p>

        <p class="math">
            \begin{aligned}
            \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_{\text{gen}}} \left[ \frac{\frac{\pi_\theta(\tau)}{\pi_{\text{gen}}(\tau)}}{\mathbb{E}_{\tau' \sim \pi_{\text{gen}}} \left[ \frac{\pi_\theta(\tau')}{\pi_{\text{gen}}(\tau')} \right]} \nabla_\theta \log \pi_\theta(\tau) R(\tau) \right]
            \end{aligned}
        </p>

        <p>In practice, for a batch of trajectories, one would estimate the denominator using the mean of the importance sampling weights for the batch. Though this weighting scheme is biased (unlike importance sampling which replaces the denominator with 1), it is consistent (i.e. it is unbiased in the limit of infinite samples). Additionally, it is often much lower variance than the importance sampled policy gradient. </p>

        <br>

        <p>One hope is that this sampler is lower variance than no clipping but is less biased than clipping. Not good enough unfortunately. TODO EMPIRICAL RESULTS. </p>

        <h3>Potential idea: Robust mean estimaton for the gradient</h3>

        <p>TODO MAYBE, doesn't work</p>

        <h3>Potential idea: KL regularization with a reference policy</h3>

        <p>TODO, reduces variances but also leads to the wrong loss minimizer</p>

        <h3>Value-based RL</h3>

        <p>TLDR: minimize a consistency objective over data generated by anything. Unclear how it stacks up in practice.</p>

        <h3>Token level</h3>

        <p>TODO just importance sample at the token level not the trajectory level</p>

        <p>TODO EMPIRICAL RESULTS. </p>

        <h3>Appendix</h3>

        <h4 id="app-a">A: Expectation of log-gradient is zero</h4>

        <p>One derivation that comes up a lot is showing $\mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \nabla_\theta \log \pi_\theta(\tau) \right] = 0$. Writing it out for personal reference:</p>

        <p class="math">
            \begin{aligned}
            \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \nabla_\theta \log \pi_\theta(\tau) \right] &= \int_\tau \nabla_\theta \log \pi_\theta(\tau) \pi_\theta(\tau) d\tau && \text{[definition]} \\
            &= \int_\tau \frac{\nabla_\theta \pi_\theta(\tau)}{\pi_\theta(\tau)} d\tau && \text{[log-gradient trick]} \\
            &= \nabla_\theta \int_\tau \pi_\theta(\tau) d\tau && \text{[derivative of constant]} \\
            &= \nabla_\theta 1 && \text{[integral of density is 1]} \\
            &= 0 && \text{[derivative of constant]} \\
            \end{aligned}
        </p>

        <h4 id="app-b">B: Estimating the gradient of the KL divergence</h4>

        <p>One common way to increase stability (while changing your target policy) is to modify the objective by adding KL regularization with respect to a reference policy. Interestingly, according to <cite><a href="https://arxiv.org/abs/2506.09477">Tang and Munos, 2025</a></cite>, most people do this wrong in practice. To do it right, let's first find the derivative of the KL divergence. </p>

        <p class="math">
            \begin{aligned}
            & \nabla_\theta D_{KL}(\pi_\theta || \pi_{\text{ref}}) \\
            &= \nabla_\theta \mathbb{E}_{\tau \sim \pi_{\theta}} \left[  \log \pi_\theta(\tau) - \log \pi_{\text{ref}}(\tau) \right] && \text{[definition]} \\
            &= \int_\tau \nabla_\theta \left( \pi_\theta(\tau) \left(  \log \pi_\theta(\tau) - \log \pi_{\text{ref}}(\tau) \right) \right) && \text{[unwrap expectation]} \\
            &= \int_\tau \nabla_\theta \pi_\theta(\tau) \left(  \log \pi_\theta(\tau) - \log \pi_{\text{ref}}(\tau) \right) + \int_\tau \pi_\theta(\tau) \nabla_\theta \log \pi_\theta(\tau) && \text{[product rule]} \\
            &= \int_\tau \nabla_\theta \pi_\theta(\tau) \left(  \log \pi_\theta(\tau) - \log \pi_{\text{ref}}(\tau) \right) && \text{[Appendix A]} \\
            &= \int_\tau \pi_\theta(\tau) \nabla_\theta \log \pi_\theta(\tau) \left(  \log \pi_\theta(\tau) - \log \pi_{\text{ref}}(\tau) \right) && \text{[log-gradient trick]} \\
            &= \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \nabla_\theta \log \pi_\theta(\tau) \left(  \log \pi_\theta(\tau) - \log \pi_{\text{ref}}(\tau) \right) \right] && \text{[wrap expectation]} \\
            \end{aligned}
        </p>

        <p>Similar to how there are two ways to set up the reward for the policy gradient, there are two quantities that yield the same KL gradient.</p>

        <ol>
            <li>Squared gradient estimator: $\mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \frac{1}{2} \left(\log \pi_\theta(\tau) - \log \pi_{\text{ref}}(\tau) \right)^2 \right]$
            <br>
            This estimator, introduced by John Schulman in his blog, is actually a biased estimator of the KL divergence and achieves low error when the policies are close to each other <cite>(<a href="https://joschu.net/blog/kl-approx.html, 2017">Schulman et al, 2017</a>)</cite>. Interestingly, it is always an unbiased estimator of the KL divergence.</li>
            <li>Surrogate KL: $\mathbb{E}_{\tau \sim \pi_{\theta}} \left[\log\pi_\theta(\tau) \texttt{ detach}\left(\log\pi_\theta(\tau) - \log\pi_{\text{ref}}(\tau)\right)\right]$
            <br>This estimator exploits autograd for a simpler expression, similar to our surrogate reward for policy gradient.</li>
        </ol>

        <p>Note that the naive unbiased estimator of $\mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \log \pi_\theta(\tau) - \log \pi_{\text{ref}}(\tau) \right]$ does not yield the correct derivative: in fact, its value is zero by Appendix A.</p>

        <br>

        <p>These unbiased estimators can incur high variance. Therefore, we can add baselines to reduce variance. For example, we can normalize $\log \pi_\theta(\tau) - \log \pi_{\text{ref}}(\tau)$ by its expected value using leave-one-out. </p>

        <br>

        <p>Note that both of these estimators require sampling from the reference policy. To make these principled, need to apply importance sampling, TODO but that incurs high variance. TODO how to solve this?</p>

        <!-- <h3>Conclusion</h3>

        <p>Data augmentation is a powerful tool for leveraging invariances in data unknown to the training process. Hopefully, we can make it more scalable by leveraging structure in the augmentation distribution, like we can for permutation invariance. Thank you for reading, and feel free to reach out with any questions or thoughts!</p>         -->

        <script type="text/javascript" src="../js/post.js"></script>
    </div>
</body>

</html>


<!-- publish link: http://www.andrew.cmu.edu/server/publish.html -->